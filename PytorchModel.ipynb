{
 "cells": [
  {
   "cell_type": "raw",
   "id": "2047315b-36c2-4d8f-a83c-63879ebc12bd",
   "metadata": {},
   "source": [
    "!pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fbf4c6eb-25ed-4203-8648-eeba3785d666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "\n",
    "import nltk\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import torchtext.data as data\n",
    "import torchtext.vocab as vocab\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import gensim\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1984036-260d-4f74-8cdd-2831a9362abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "16  CPUs available\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print('Device:', device)\n",
    "\n",
    "print(mp.cpu_count(),' CPUs available')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d79ce19-7603-4f21-86e0-2157e3ccf07a",
   "metadata": {},
   "source": [
    "## Embeddings for text\n",
    "Can use Pre-trained embeddings from gensim (https://www.scaler.com/topics/pytorch/text-representation-pytorch/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff696553-ec57-487c-ae4c-6f59b15c5e69",
   "metadata": {},
   "source": [
    "Let's load up a big trained version of word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc7675db-2ab1-436d-8dff-88596bdda255",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.1 s, sys: 2.62 s, total: 24.7 s\n",
      "Wall time: 24.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "w2v = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9379c6af-9440-47c9-97d7-1dc30b3a1bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opening the file: train/00066EA9880D.txt\n"
     ]
    }
   ],
   "source": [
    "# read a text file\n",
    "filelist = os.listdir('train')\n",
    "filepath = 'train/'+filelist[1]\n",
    "\n",
    "# read the raw text file\n",
    "print('opening the file:', filepath)\n",
    "with open(filepath, 'r', encoding='utf-8') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ea3b1089-ccb5-48e8-941c-9b2a3723822a",
   "metadata": {},
   "source": [
    "!pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcc65eb3-a794-49d9-98e1-8beef14d9c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize into words and remove punctuation\n",
    "# using the pre-processing from the spell-check code\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import contractions #library pertaining to contractions (things like \"don't\" and \"you're\")\n",
    "\n",
    "def preprocess_spelling(essay):\n",
    "    tokenizer = RegexpTokenizer(r'[a-zA-Z0-9]+') #This tokenises strings that consist of characters and number, i.e. it removes other symbols\n",
    "    text_no_contr = contractions.fix(essay)  #Expands all contractions in essays. For example, converts \"you're\" to \"you are\".\n",
    "    words_no_punct= tokenizer.tokenize(text_no_contr.lower())  #Removes all non-letter and non-number symbols. Also makes everything lowercase.\n",
    "    return words_no_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3227612-8e9d-44ec-b4ea-640e6e94b661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "648"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essay_words = preprocess_spelling(text)\n",
    "len(essay_words)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ea69449a-61eb-4a7b-a40e-e465e18ba7c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "w2v.get_vector('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b19e15dd-ad70-4f3b-ab90-1fe51cb02b71",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(571, 300)\n",
      "CPU times: user 1.73 ms, sys: 0 ns, total: 1.73 ms\n",
      "Wall time: 1.41 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['100', 'a', 'and', 'judgement', 'malfucntion', 'malfunciton', 'of',\n",
       "       'techological', 'to'], dtype='<U12')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "vecs = []\n",
    "missing_words =[]\n",
    "for word in essay_words:\n",
    "    try:\n",
    "        vec = w2v.get_vector(word)\n",
    "        vecs.append(vec)\n",
    "    except KeyError:\n",
    "        # this means that there is a misspelling or the word is too generic\n",
    "        missing_words.append(word)\n",
    "vecs = np.array(vecs)\n",
    "print(vecs.shape)\n",
    "\n",
    "# all the words that are missing\n",
    "np.unique(np.array(missing_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbb832a-c311-4a28-97c9-34c53d55db25",
   "metadata": {},
   "source": [
    "## Essay Metrics\n",
    "Various measurements taken for a given set of text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0f3752-db85-4a88-bb65-d162ef6848d1",
   "metadata": {},
   "source": [
    "### Burstiness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "81f6288b-3c7a-4dc7-b973-d5ba3f3fd5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/rachel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/rachel/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5324ede6-d0ff-4744-aeb9-d738cefb7d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_word(essay):\n",
    "    tokens = nltk.word_tokenize(essay.lower())\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words and token not in string.punctuation]\n",
    "    return tokens\n",
    "def token_sent(essay):\n",
    "    tokens = nltk.sent_tokenize(essay.lower())\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "    return tokens\n",
    "\n",
    "# The following function calculates burstiness of an essay\n",
    "def burstiness(essay):\n",
    "    sentences = token_sent(essay)\n",
    "    num_words   = len(token_word(essay))  #Total number of words in text\n",
    "    num_sents   = len(sentences)  #Total number of sentences in text\n",
    "    avg_freq = num_words/num_sents #Average number of words per sentence \n",
    "    variance = sum((len(sentence.split()) - avg_freq) ** 2 for sentence in sentences) / len(sentences)\n",
    "    return variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "99488ab7-eb95-40ea-9f47-df42d6ea7b63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205.62666666666667"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "burstiness(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872ad13d-e1b0-4ab7-a85b-1915dcb49c01",
   "metadata": {},
   "source": [
    "### Seniment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6b8f25e8-ef05-4bed-93b3-2953778b7f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e0cdd0ca-8363-49da-ba87-100be076c0f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.121, 'neu': 0.844, 'pos': 0.035, 'compound': -0.9946}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sia_scores = sia.polarity_scores(text)\n",
    "sia_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087751c6-a334-4206-8f9d-7e5bc1438ec3",
   "metadata": {},
   "source": [
    "### Put it all in an Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b90909f3-0850-4fb5-841d-09eea4481c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.05626667e+02  3.50000000e-02  1.21000000e-01  8.44000000e-01\n",
      " -9.94600000e-01]\n"
     ]
    }
   ],
   "source": [
    "metrics = np.array([burstiness(text), sia_scores['pos'] , sia_scores['neg'], sia_scores['neu'], sia_scores['compound']])\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f2ab7e-be67-45c9-b627-be72037e41d8",
   "metadata": {},
   "source": [
    "## Define a Model\n",
    "https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "c032fe72-3d5e-457d-8d43-2843e52e33ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, N_text_layers=1, N_text_in = 300, N_text_out=128, use_LSTM=False, N_metrics=5):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        # RNN or LSTM part\n",
    "        if use_LSTM:\n",
    "            self.text_read = nn.LSTM(input_size= N_text_in, hidden_size=N_text_out, num_layers=N_text_layers)\n",
    "        else:\n",
    "            self.text_read = nn.RNN(input_size= N_text_in, hidden_size=N_text_out, num_layers=N_text_layers)\n",
    "        # linear NN part\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(N_text_out + N_metrics, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 2), # last layer is outputting probabilities\n",
    "        )\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x, metrics):\n",
    "        #x = self.flatten(x)\n",
    "        \n",
    "        # do the RNN or LSTM part\n",
    "        # TODO: look into adding attention mechanism\n",
    "        text_out = self.text_read(x)\n",
    "        \n",
    "        # concatenate\n",
    "        x2 = torch.cat((text_out[1], metrics), axis=1)\n",
    "        # text_out[1] is the final hidden state for each element in the batch.\n",
    "        \n",
    "        # put both the metric array \n",
    "        logits = self.linear_relu_stack(x2)\n",
    "        \n",
    "        # softmax to return probabilities\n",
    "        return self.softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3411c4-3d0a-44e7-bec9-2ac9a605d78f",
   "metadata": {},
   "source": [
    "### Check that we can pass our inputs into our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "0a8db9fa-2fbe-4897-ba62-3411dafd9d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (text_read): RNN(300, 128)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=133, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=512, out_features=2, bias=True)\n",
       "  )\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create model\n",
    "model = Classifier(N_metrics=len(metrics))\n",
    "model = model.to(device)# put it on gpu\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "28768c92-3db6-4c81-bf38-39701270fb53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([571, 300]), torch.Size([1, 5]))"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert to pytorch tensors and put them on gpu\n",
    "text_in = torch.from_numpy(vecs).to(device).reshape(-1,300)\n",
    "met_in =  torch.from_numpy(metrics).to(device).reshape(1,-1).float()\n",
    "text_in.shape, met_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "b8a75d59-b3ef-4dfb-8385-0049e4934151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.text_read(text_in)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "6fb1b49b-7052-433d-85de-8297dee91970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1907, 0.8093]], device='cuda:0', grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(text_in,met_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fac2797-309a-4ce3-93fd-4c181e05588f",
   "metadata": {},
   "source": [
    "That worked and gave us two values between 0 and 1 like we wanted. I want to quickly make use it handles batches correctly.\n",
    "\n",
    "### Inputting batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "58212b98-db69-4199-89df-7754829ab214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opening the file: train/000E6DE9E817.txt\n",
      "274\n"
     ]
    }
   ],
   "source": [
    "# read a text file\n",
    "filelist = os.listdir('train')\n",
    "filepath = 'train/'+filelist[2]\n",
    "\n",
    "# read the raw text file\n",
    "print('opening the file:', filepath)\n",
    "with open(filepath, 'r', encoding='utf-8') as file:\n",
    "    text2 = file.read()\n",
    "    \n",
    "essay_words2 = preprocess_spelling(text2)\n",
    "print(len(essay_words2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "32cfa670-d548-4bf2-9967-04077f39e535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(244, 300)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['a', 'and', 'incasing', 'of', 'to'], dtype='<U8')"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs2 = []\n",
    "missing_words2 =[]\n",
    "for word in essay_words2:\n",
    "    try:\n",
    "        vec = w2v.get_vector(word)\n",
    "        vecs2.append(vec)\n",
    "    except KeyError:\n",
    "        # this means that there is a misspelling or the word is too generic\n",
    "        missing_words2.append(word)\n",
    "vecs2 = np.array(vecs2)\n",
    "print(vecs2.shape)\n",
    "\n",
    "# all the words that are missing\n",
    "np.unique(np.array(missing_words2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "ab471e35-95f1-4788-9da6-287923064bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia_scores2 = sia.polarity_scores(text2)\n",
    "metrics2 = np.array([burstiness(text2), sia_scores2['pos'] , sia_scores2['neg'], sia_scores2['neu'], sia_scores2['compound']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "fb5e7955-b9ef-417f-8697-9a8bececca7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to pytorch tensors and put them on gpu\n",
    "text_in2 = torch.from_numpy(vecs2).to(device).reshape(-1,300)\n",
    "met_in2 =  torch.from_numpy(metrics2).to(device).reshape(1,-1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "6c931044-faf6-4244-ba9a-1933e1369cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_in_batch = torch.cat((text_in,text_in2), axis=0)\n",
    "met_in_batch = torch.cat((met_in, met_in2), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "2915cb1a-8c67-4bd1-8019-4960b231d9ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([571, 300]), torch.Size([244, 300]))"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_in.shape, text_in2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "7973f633-61f0-4c3d-98eb-4a8ff429e992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "met_in_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "b2191798-6959-490c-a386-c6b8bdbea9ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([815, 300])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_in_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f559da-5b34-4d9d-a847-14f9bcf9aa38",
   "metadata": {},
   "source": [
    "Oh wait, this isn't right because we need to use padding to ensure that the essays are \"all the same length\" if we want to put things into batches. I'll come back to this, but we have an initial model that can (theoretically) be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb652740-c255-4b8b-bf6a-fcd1e771a241",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
