{
 "cells": [
  {
   "cell_type": "raw",
   "id": "2047315b-36c2-4d8f-a83c-63879ebc12bd",
   "metadata": {},
   "source": [
    "!pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbf4c6eb-25ed-4203-8648-eeba3785d666",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rachel/anaconda3/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "\n",
    "import nltk\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import torchtext.data as data\n",
    "import torchtext.vocab as vocab\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import gensim\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "672187c4-2615-4e6e-a7fa-a3f19e8aba9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"1.0\",\"1.0\",\"1.0\",\"1.0\",\"1.0\",'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.ones(5)\n",
    "('\"%s\",'*5)%tuple(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1984036-260d-4f74-8cdd-2831a9362abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "16  CPUs available\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print('Device:', device)\n",
    "\n",
    "print(mp.cpu_count(),' CPUs available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9379c6af-9440-47c9-97d7-1dc30b3a1bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opening the file: data/train/00D6341E8688.txt\n"
     ]
    }
   ],
   "source": [
    "# read a text file\n",
    "filelist = os.listdir('data/train')\n",
    "filepath = 'data/train/'+filelist[50]\n",
    "\n",
    "# read the raw text file\n",
    "print('opening the file:', filepath)\n",
    "with open(filepath, 'r', encoding='utf-8') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191e2e84-384b-4be8-abc1-b1535bae8dd8",
   "metadata": {},
   "source": [
    "## Essay Metrics\n",
    "Various measurements taken for a given set of text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc858a3-99ca-4ae8-bc2d-d9d3285ac411",
   "metadata": {},
   "source": [
    "### Burstiness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81f6288b-3c7a-4dc7-b973-d5ba3f3fd5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/rachel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/rachel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5324ede6-d0ff-4744-aeb9-d738cefb7d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_word(essay):\n",
    "    tokens = nltk.word_tokenize(essay.lower())\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words and token not in string.punctuation]\n",
    "    return tokens\n",
    "def token_sent(essay):\n",
    "    tokens = nltk.sent_tokenize(essay.lower())\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "    return tokens\n",
    "\n",
    "# The following function calculates burstiness of an essay\n",
    "def burstiness(essay):\n",
    "    sentences = token_sent(essay)\n",
    "    num_words   = len(token_word(essay))  #Total number of words in text\n",
    "    num_sents   = len(sentences)  #Total number of sentences in text\n",
    "    avg_freq = num_words/num_sents #Average number of words per sentence \n",
    "    variance = sum((len(sentence.split()) - avg_freq) ** 2 for sentence in sentences) / len(sentences)\n",
    "    return variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "99488ab7-eb95-40ea-9f47-df42d6ea7b63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103.63905325443787"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "burstiness(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da53baee-e8cd-4633-a7a4-cb85a3dd66bc",
   "metadata": {},
   "source": [
    "### Seniment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6b8f25e8-ef05-4bed-93b3-2953778b7f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e0cdd0ca-8363-49da-ba87-100be076c0f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.065, 'neu': 0.845, 'pos': 0.09, 'compound': 0.8956}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sia_scores = sia.polarity_scores(text)\n",
    "sia_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5288ad-461f-4f1c-b8e4-845488887d82",
   "metadata": {},
   "source": [
    "### Put it all in an Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b90909f3-0850-4fb5-841d-09eea4481c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.03639053e+02 9.00000000e-02 6.50000000e-02 8.45000000e-01\n",
      " 8.95600000e-01]\n"
     ]
    }
   ],
   "source": [
    "metrics = np.array([burstiness(text), sia_scores['pos'] , sia_scores['neg'], sia_scores['neu'], sia_scores['compound']])\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35249826-c2b5-4103-8937-c62236dce573",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Numbers to words https://github.com/collin5/python-n2w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b28bcd-68ef-4baf-9293-4d659bbe20c4",
   "metadata": {},
   "source": [
    "### Embeddings for text\n",
    "Can use Pre-trained embeddings from gensim (https://www.scaler.com/topics/pytorch/text-representation-pytorch/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c70a635-3f2e-4857-ae78-d56aa33dffc9",
   "metadata": {},
   "source": [
    "Let's load up a big trained version of word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f2746220-f6be-4b7b-bb90-92efa8d1e04f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.6 s, sys: 687 ms, total: 22.3 s\n",
      "Wall time: 22.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "w2v = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4f965f20-4d87-443c-af30-71208de44f23",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "w2v.get_vector('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d5a566-a655-4c45-9185-bcdebe7f7c46",
   "metadata": {},
   "source": [
    "### Spell corrections and punctuation removal"
   ]
  },
  {
   "cell_type": "raw",
   "id": "06616f50-adde-49b7-b15b-9ae270798514",
   "metadata": {},
   "source": [
    "! pip install n2w"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6cec3295-d789-4753-aa6e-017f97179b7a",
   "metadata": {},
   "source": [
    "!pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dcc65eb3-a794-49d9-98e1-8beef14d9c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /home/rachel/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# tokenize into words and remove punctuation\n",
    "# using the pre-processing from the spell-check code\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import contractions #library pertaining to contractions (things like \"don't\" and \"you're\")\n",
    "from nltk.metrics.distance import jaccard_distance #distance we'll use to find the nearest correct word\n",
    "from nltk.util import ngrams\n",
    "\n",
    "nltk.download('words') #nltk's collection of words\n",
    "from nltk.corpus import words \n",
    "from nltk.metrics.distance import jaccard_distance #distance we'll use to find the nearest correct word\n",
    "from nltk.util import ngrams\n",
    "\n",
    "import n2w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "be79741b-3cc5-453c-ace8-c2d165c50d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code loads the list of most common english words and adds numbers to it as I assume \n",
    "# we don't want to count the use of numbers as a spelling mistake.\n",
    "\n",
    "words_into_list = words.words() #this is a text file with 1 word per line\n",
    "words_into_list = words_into_list+[str(i) for i in range(0,1000000)] #add numbers to the list\n",
    "words_lower = [word.lower() for word in words_into_list] #we will make all words lowercase\n",
    "word_set = set(words_lower)\n",
    "word_arr = np.array(list(word_set))\n",
    "\n",
    "def preprocess_spelling(essay):\n",
    "    tokenizer = RegexpTokenizer(r'[a-zA-Z0-9]+') #This tokenises strings that consist of characters and number, i.e. it removes other symbols\n",
    "    text_no_contr = contractions.fix(essay)  #Expands all contractions in essays. For example, converts \"you're\" to \"you are\".\n",
    "    words_no_punct= tokenizer.tokenize(text_no_contr.lower())  #Removes all non-letter and non-number symbols. Also makes everything lowercase.\n",
    "    return words_no_punct\n",
    "\n",
    "\n",
    "def corrected(essay):\n",
    "    errors=[]\n",
    "    clean_essay = preprocess_spelling(essay)\n",
    "    correct_essay_words = clean_essay\n",
    "    for word_index in range(0,len(clean_essay)): #loop through words in each essay\n",
    "        word = clean_essay[word_index]\n",
    "        if word not in word_set: #if a word is not contained in our word_set, correct it using jaccard\n",
    "            temp = [(jaccard_distance(set(ngrams(word, 2)), set(ngrams(w, 2))),w) \n",
    "            for w in word_set if w[0]==word[0]] \n",
    "            correct_word = sorted(temp, key = lambda val:val[0])[0][1] #corrected word\n",
    "            correct_essay_words[word_index] = correct_word\n",
    "    correct_essay = \" \".join(correct_essay_words)\n",
    "    return correct_essay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "329baa5f-a5fc-4e8c-9159-68730b58357b",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_into_list = words.words() #this is a text file with 1 word per line\n",
    "#words_into_list = words_into_list+[str(i) for i in range(0,1000000)] #add numbers to the list\n",
    "words_lower = [word.lower() for word in words_into_list] #we will make all words lowercase\n",
    "word_set = set(words_lower)\n",
    "word_arr = np.array(list(word_set))\n",
    "\n",
    "\n",
    "def preprocess(essay, word_tokenize=False):\n",
    "    \"\"\"\n",
    "    Do a lot of the pre-processing of the text\n",
    "    input:\n",
    "        essay (string)\n",
    "    output:\n",
    "        correct_essay_words (list of strings) cleaned/corrected essay words in order\n",
    "    \"\"\"\n",
    "    \n",
    "    clean_essay = preprocess_spelling(essay)\n",
    "    correct_essay_words = clean_essay\n",
    "    \n",
    "    # use numpy to avoid the nested loops\n",
    "    \n",
    "    # find the indices where the words arent in the word set\n",
    "    missing_word_inds = np.where(~np.isin(np.array(clean_essay),word_arr))[0]\n",
    "    \n",
    "    for word_index in missing_word_inds:\n",
    "        word = clean_essay[word_index]\n",
    "\n",
    "        \n",
    "        try: \n",
    "            # first check if the word is in word2vec\n",
    "            w2v.get_vector(word)\n",
    "        except KeyError:\n",
    "            # check if it's a number first\n",
    "            num = n2w.convert(word)\n",
    "            if num=='Input not a valid number':\n",
    "                temp = [(jaccard_distance(set(ngrams(word, 2)), set(ngrams(w, 2))),w) for w in word_set if w[0]==word[0]]\n",
    "                correct_word = sorted(temp, key = lambda val:val[0])[0][1] #corrected word\n",
    "                correct_essay_words[word_index] = correct_word\n",
    "            else:\n",
    "                correct_essay_words[word_index] = num\n",
    "            \n",
    "    if word_tokenize:\n",
    "        return correct_essay_words\n",
    "    else:\n",
    "        correct_essay = \" \".join(correct_essay_words)\n",
    "        return correct_essay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4149f5a5-4b69-4b2a-b897-a5c606425734",
   "metadata": {},
   "outputs": [],
   "source": [
    "remap = {'a':'the', 'and':'also', 'of':'in', 'to':'at'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7a986d39-3532-4d28-8aa1-28032371b709",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 972 ms, sys: 10 ms, total: 982 ms\n",
      "Wall time: 982 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "processed_text = preprocess(text)\n",
    "\n",
    "processed_text_words = processed_text.split()# split by whitespace\n",
    "essay_words = [remap[word] if word in remap.keys() else word for word in processed_text_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b19e15dd-ad70-4f3b-ab90-1fe51cb02b71",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(462, 300)\n",
      "missing ['addu' 'aldus' 'bogota' 'eveweed' 'lichenes' 'milliad' 'stret']\n",
      "CPU times: user 0 ns, sys: 6.87 ms, total: 6.87 ms\n",
      "Wall time: 6.49 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vecs = []\n",
    "missing_words =[]\n",
    "for word in essay_words:\n",
    "    try:\n",
    "        vec = w2v.get_vector(word)\n",
    "        vecs.append(vec)\n",
    "    except KeyError:\n",
    "        # this means that the word isn't in the w2v\n",
    "        missing_words.append(word)\n",
    "vecs = np.array(vecs)\n",
    "print(vecs.shape)\n",
    "\n",
    "#print all the words that are missing\n",
    "print('missing', np.unique(np.array(missing_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f2ab7e-be67-45c9-b627-be72037e41d8",
   "metadata": {},
   "source": [
    "## Define a Model\n",
    "https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c032fe72-3d5e-457d-8d43-2843e52e33ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, N_text_layers=1, N_text_in = 300, N_text_out=128, use_LSTM=False, N_metrics=5):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        # RNN or LSTM part\n",
    "        if use_LSTM:\n",
    "            self.text_read = nn.LSTM(input_size= N_text_in, hidden_size=N_text_out, num_layers=N_text_layers)\n",
    "        else:\n",
    "            self.text_read = nn.RNN(input_size= N_text_in, hidden_size=N_text_out, num_layers=N_text_layers)\n",
    "        # linear NN part\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(N_text_out + N_metrics, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 2), # last layer is outputting probabilities\n",
    "        )\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x, metrics):\n",
    "        #x = self.flatten(x)\n",
    "        \n",
    "        # do the RNN or LSTM part\n",
    "        # TODO: look into adding attention mechanism\n",
    "        text_out = self.text_read(x)\n",
    "        \n",
    "        # concatenate\n",
    "        x2 = torch.cat((text_out[1], metrics), axis=1)\n",
    "        # text_out[1] is the final hidden state for each element in the batch.\n",
    "        \n",
    "        # put both the metric array \n",
    "        logits = self.linear_relu_stack(x2)\n",
    "        \n",
    "        # softmax to return probabilities\n",
    "        return self.softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3411c4-3d0a-44e7-bec9-2ac9a605d78f",
   "metadata": {},
   "source": [
    "### Check that we can pass our inputs into our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0a8db9fa-2fbe-4897-ba62-3411dafd9d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (text_read): RNN(300, 128)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=133, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=512, out_features=2, bias=True)\n",
       "  )\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create model\n",
    "model = Classifier(N_metrics=len(metrics))\n",
    "model = model.to(device)# put it on gpu\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "28768c92-3db6-4c81-bf38-39701270fb53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([330, 300]), torch.Size([1, 5]))"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert to pytorch tensors and put them on gpu\n",
    "text_in = torch.from_numpy(vecs).to(device).reshape(-1,300)\n",
    "met_in =  torch.from_numpy(metrics).to(device).reshape(1,-1).float()\n",
    "text_in.shape, met_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b8a75d59-b3ef-4dfb-8385-0049e4934151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.text_read(text_in)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6fb1b49b-7052-433d-85de-8297dee91970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9047, 0.0953]], device='cuda:0', grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(text_in,met_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fac2797-309a-4ce3-93fd-4c181e05588f",
   "metadata": {},
   "source": [
    "That worked and gave us two values between 0 and 1 like we wanted. I want to quickly make use it handles batches correctly.\n",
    "\n",
    "### Inputting batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "58212b98-db69-4199-89df-7754829ab214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opening the file: train/000E6DE9E817.txt\n",
      "274\n"
     ]
    }
   ],
   "source": [
    "# read a text file\n",
    "filelist = os.listdir('train')\n",
    "filepath = 'train/'+filelist[2]\n",
    "\n",
    "# read the raw text file\n",
    "print('opening the file:', filepath)\n",
    "with open(filepath, 'r', encoding='utf-8') as file:\n",
    "    text2 = file.read()\n",
    "    \n",
    "essay_words2 = preprocess_spelling(text2)\n",
    "print(len(essay_words2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "32cfa670-d548-4bf2-9967-04077f39e535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(244, 300)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['a', 'and', 'incasing', 'of', 'to'], dtype='<U8')"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs2 = []\n",
    "missing_words2 =[]\n",
    "for word in essay_words2:\n",
    "    try:\n",
    "        vec = w2v.get_vector(word)\n",
    "        vecs2.append(vec)\n",
    "    except KeyError:\n",
    "        # this means that there is a misspelling or the word is too generic\n",
    "        missing_words2.append(word)\n",
    "vecs2 = np.array(vecs2)\n",
    "print(vecs2.shape)\n",
    "\n",
    "# all the words that are missing\n",
    "np.unique(np.array(missing_words2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "ab471e35-95f1-4788-9da6-287923064bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia_scores2 = sia.polarity_scores(text2)\n",
    "metrics2 = np.array([burstiness(text2), sia_scores2['pos'] , sia_scores2['neg'], sia_scores2['neu'], sia_scores2['compound']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "fb5e7955-b9ef-417f-8697-9a8bececca7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to pytorch tensors and put them on gpu\n",
    "text_in2 = torch.from_numpy(vecs2).to(device).reshape(-1,300)\n",
    "met_in2 =  torch.from_numpy(metrics2).to(device).reshape(1,-1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "6c931044-faf6-4244-ba9a-1933e1369cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_in_batch = torch.cat((text_in,text_in2), axis=0)\n",
    "met_in_batch = torch.cat((met_in, met_in2), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "2915cb1a-8c67-4bd1-8019-4960b231d9ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([571, 300]), torch.Size([244, 300]))"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_in.shape, text_in2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "7973f633-61f0-4c3d-98eb-4a8ff429e992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "met_in_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "b2191798-6959-490c-a386-c6b8bdbea9ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([815, 300])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_in_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f559da-5b34-4d9d-a847-14f9bcf9aa38",
   "metadata": {},
   "source": [
    "Oh wait, this isn't right because we need to use padding to ensure that the essays are \"all the same length\" if we want to put things into batches. I'll come back to this, but we have an initial model that can (theoretically) be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb652740-c255-4b8b-bf6a-fcd1e771a241",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
