{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spelling correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes for Rachel:\n",
    "\n",
    "Here I give two options: \n",
    "\n",
    "1. A function that takes an essay and returns a corrected essay with no full stops, in lowercase and with contractions expaned.\n",
    "2. A function that takes an essay and returns a corrected essay with full stops, in lowercase and with contractions expaned.\n",
    "\n",
    "I give explanations below. Scroll all the way down for just the code for both options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\s1557452\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import contractions #library pertaining to contractions (things like \"don't\" and \"you're\")\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer #we'll use this to remove non-number and non-letter symbols\n",
    "\n",
    "\n",
    "nltk.download('words') #nltk's collection of words\n",
    "from nltk.corpus import words \n",
    "\n",
    "from nltk.metrics.distance import jaccard_distance #distance we'll use to find the nearest correct word\n",
    "from nltk.util import ngrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay = \"This is an essay. This is a mistakke. Here's a contraction\"\n",
    "#this is our example essay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# The following code preprocesses an essay. Corresponding to each essay we now have a list of \n",
    "# words that are lowercase, all contractions are expanded, and they do not contain no non-number and \n",
    "# non-letter symbols.\n",
    "\n",
    "def preprocess_spelling(essay): #returns a list of words\n",
    "    tokenizer = RegexpTokenizer(r'[a-zA-Z0-9]+') #This tokenises strings that consist of characters and numbers, i.e. it removes other symbols\n",
    "    text_no_contr = contractions.fix(essay)  #Expands all contractions in essays. For example, converts \"you're\" to \"you are\".\n",
    "    words_no_punct= tokenizer.tokenize(text_no_contr.lower())  #Removes all non-letter and non-number symbols. Also makes everything lowercase.\n",
    "    return words_no_punct\n",
    "\n",
    "\n",
    "\n",
    "# The following code loads the list of most common english words and adds numbers to it as I assume \n",
    "# we don't want to count the use of numbers as a spelling mistake.\n",
    "\n",
    "words_into_list = words.words() #this is a text file with 1 word per line\n",
    "words_into_list = words_into_list+[str(i) for i in range(0,1000000)] #add numbers to the list\n",
    "words_lower = [word.lower() for word in words_into_list] #we will make all words lowercase\n",
    "word_set = set(words_lower)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function takes an essay and returns the list of misspelled words.\n",
    "\n",
    "def misspellings(essay):\n",
    "    errors=[]\n",
    "    clean_essay = preprocess_spelling(essay)\n",
    "    for word in clean_essay: #loop through words in each essay\n",
    "        if word not in word_set: #if a word is not contained in our word_set, add to errors\n",
    "            errors.append(word)\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mistakke']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misspellings(essay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function returns a corrrected essay without punctuation and in lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a function that takes in an essay and returns the corrected essay, but without punctuation and in lowercase\n",
    "\n",
    "def corrected(essay):\n",
    "    errors=[]\n",
    "    clean_essay = preprocess_spelling(essay)\n",
    "    correct_essay_words = clean_essay\n",
    "    for word_index in range(0,len(clean_essay)): #loop through words in each essay\n",
    "        word = clean_essay[word_index]\n",
    "        if word not in word_set: #if a word is not contained in our word_set, correct it using jaccard\n",
    "            temp = [(jaccard_distance(set(ngrams(word, 2)), set(ngrams(w, 2))),w) \n",
    "            for w in word_set if w[0]==word[0]] \n",
    "            correct_word = sorted(temp, key = lambda val:val[0])[0][1] #corrected word\n",
    "            correct_essay_words[word_index] = correct_word\n",
    "    correct_essay = \" \".join(correct_essay_words)\n",
    "    return correct_essay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is an essay this is a mistake here is a contraction'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "corrected(essay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function returns a corrected essay with punctuation and in lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we preprocess the text so that we keep sentence structure.\n",
    "\n",
    "def preprocess_sent(text):\n",
    "    text_no_contr = contractions.fix(text)  #Expands all contractions in essays. For example, converts \"you're\" to \"you are\".\n",
    "    tokens = text_no_contr.split(\".\")\n",
    "    tokens = [token.lower() for token in tokens if token not in string.punctuation]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is an essay', ' this is a mistakke', ' here is a contraction']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_sent(essay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function takes in an essay and returns a corrected version in lowercase with punctuation and expanded contractions.\n",
    "\n",
    "def corrected2(essay):\n",
    "    sentences = preprocess_sent(essay)\n",
    "    corrected_sent = [corrected(sentence) for sentence in sentences]\n",
    "    corrected_essay = \". \".join(corrected_sent)+\".\"\n",
    "    return corrected_essay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is an essay. this is a mistake. here is a contraction.'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrected2(essay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just code for no full-stops version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\s1557452\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import contractions #library pertaining to contractions (things like \"don't\" and \"you're\")\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer #we'll use this to remove non-number and non-letter symbols\n",
    "\n",
    "\n",
    "nltk.download('words') #nltk's collection of words\n",
    "from nltk.corpus import words \n",
    "\n",
    "from nltk.metrics.distance import jaccard_distance #distance we'll use to find the nearest correct word\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# The following code preprocesses an essay. Corresponding to each essay we now have a list of \n",
    "# words that are lowercase, all contractions are expanded, and they do not contain no non-number and \n",
    "# non-letter symbols.\n",
    "\n",
    "def preprocess_spelling(essay): #returns a list of words\n",
    "    tokenizer = RegexpTokenizer(r'[a-zA-Z0-9]+') #This tokenises strings that consist of characters and numbers, i.e. it removes other symbols\n",
    "    text_no_contr = contractions.fix(essay)  #Expands all contractions in essays. For example, converts \"you're\" to \"you are\".\n",
    "    words_no_punct= tokenizer.tokenize(text_no_contr.lower())  #Removes all non-letter and non-number symbols. Also makes everything lowercase.\n",
    "    return words_no_punct\n",
    "\n",
    "\n",
    "# The following code loads the list of most common english words and adds numbers to it as I assume \n",
    "# we don't want to count the use of numbers as a spelling mistake.\n",
    "\n",
    "words_into_list = words.words() #this is a text file with 1 word per line\n",
    "words_into_list = words_into_list+[str(i) for i in range(0,1000000)] #add numbers to the list\n",
    "words_lower = [word.lower() for word in words_into_list] #we will make all words lowercase\n",
    "word_set = set(words_lower)\n",
    "\n",
    "\n",
    "# this is a function that takes in an essay and returns the corrected essay, but without punctuation and in lowercase\n",
    "\n",
    "def corrected(essay):\n",
    "    errors=[]\n",
    "    clean_essay = preprocess_spelling(essay)\n",
    "    correct_essay_words = clean_essay\n",
    "    for word_index in range(0,len(clean_essay)): #loop through words in each essay\n",
    "        word = clean_essay[word_index]\n",
    "        if word not in word_set: #if a word is not contained in our word_set, correct it using jaccard\n",
    "            temp = [(jaccard_distance(set(ngrams(word, 2)), set(ngrams(w, 2))),w) \n",
    "            for w in word_set if w[0]==word[0]] \n",
    "            correct_word = sorted(temp, key = lambda val:val[0])[0][1] #corrected word\n",
    "            correct_essay_words[word_index] = correct_word\n",
    "    correct_essay = \" \".join(correct_essay_words)\n",
    "    return correct_essay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is an essay this is a mistake here is a contraction'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrected(essay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just code for version with full-stops:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\s1557452\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import contractions #library pertaining to contractions (things like \"don't\" and \"you're\")\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer #we'll use this to remove non-number and non-letter symbols\n",
    "\n",
    "\n",
    "nltk.download('words') #nltk's collection of words\n",
    "from nltk.corpus import words \n",
    "\n",
    "from nltk.metrics.distance import jaccard_distance #distance we'll use to find the nearest correct word\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# The following code preprocesses an essay. Corresponding to each essay we now have a list of \n",
    "# words that are lowercase, all contractions are expanded, and they do not contain no non-number and \n",
    "# non-letter symbols.\n",
    "\n",
    "def preprocess_spelling(essay): #returns a list of words\n",
    "    tokenizer = RegexpTokenizer(r'[a-zA-Z0-9]+') #This tokenises strings that consist of characters and numbers, i.e. it removes other symbols\n",
    "    text_no_contr = contractions.fix(essay)  #Expands all contractions in essays. For example, converts \"you're\" to \"you are\".\n",
    "    words_no_punct= tokenizer.tokenize(text_no_contr.lower())  #Removes all non-letter and non-number symbols. Also makes everything lowercase.\n",
    "    return words_no_punct\n",
    "\n",
    "\n",
    "# The following code loads the list of most common english words and adds numbers to it as I assume \n",
    "# we don't want to count the use of numbers as a spelling mistake.\n",
    "\n",
    "words_into_list = words.words() #this is a text file with 1 word per line\n",
    "words_into_list = words_into_list+[str(i) for i in range(0,1000000)] #add numbers to the list\n",
    "words_lower = [word.lower() for word in words_into_list] #we will make all words lowercase\n",
    "word_set = set(words_lower)\n",
    "\n",
    "#Here we preprocess the text so that we keep sentence structure.\n",
    "\n",
    "def preprocess_sent(text):\n",
    "    text_no_contr = contractions.fix(text)  #Expands all contractions in essays. For example, converts \"you're\" to \"you are\".\n",
    "    tokens = text_no_contr.split(\".\")\n",
    "    tokens = [token.lower() for token in tokens if token not in string.punctuation]\n",
    "    return tokens\n",
    "\n",
    "#this function takes in an essay and returns a corrected version in lowercase with punctuation and expanded contractions.\n",
    "\n",
    "def corrected2(essay):\n",
    "    sentences = preprocess_sent(essay)\n",
    "    corrected_sent = [corrected(sentence) for sentence in sentences]\n",
    "    corrected_essay = \". \".join(corrected_sent)+\".\"\n",
    "    return corrected_essay\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is an essay. this is a mistake. here is a contraction.'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrected2(essay)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
